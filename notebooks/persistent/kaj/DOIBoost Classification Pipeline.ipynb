{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "from pymongo import MongoClient\n",
    "from elasticsearch import Elasticsearch\n",
    "from collections import defaultdict\n",
    "# from habanero import Crossref, counts, cn\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the mongo collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mongo client\n",
    "client = MongoClient('mongo')\n",
    "\n",
    "# get the database\n",
    "metadatadb = client.get_database('narcis')\n",
    "\n",
    "# get the metadata collection\n",
    "metacollection = metadatadb.doiboost2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The metadata retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_retriever(collection, DOI):\n",
    "    \"\"\"\n",
    "    Retrieves metadata when an DOI is provided.\n",
    "\n",
    "    @param  Collection   The metadata collection\n",
    "    @param  string       The DOI\n",
    "    @return dict         The corresponding metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the metadata with the DOI\n",
    "    subset = collection.find({'doi': DOI}).limit(1)\n",
    "    \n",
    "    # return the first item\n",
    "    for item in subset:\n",
    "        return item\n",
    "    \n",
    "    # there was no hit for the given DOI\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the metadata that belongs to the DOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5d01137ba1ae159641df3c08'), 'publisher': None, 'issn': [{'type': 'print', 'value': '0340-1200'}, {'type': 'electronic', 'value': '1432-0770'}], 'doi': '10.1007/s00422-017-0730-1', 'license': [{'url': 'http://creativecommons.org/licenses/by/4.0', 'content-version': 'unspecified', '\"delay-in-days': None, 'date-time': '2017-09-14T00:00:00Z'}], 'published-print': '2017-12-1', 'title': ['Affective–associative two-process theory: a neurocomputational account of partial reinforcement extinction effects'], 'issued': '2017-9-14', 'abstract': [{'provenance': 'MAG', 'value': 'The partial reinforcement extinction effect (PREE) is an experimentally established phenomenon: behavioural response to a given stimulus is more persistent when previously inconsistently rewarded than when consistently rewarded. This phenomenon is, however, controversial in animal/human learning theory. Contradictory findings exist regarding when the PREE occurs. One body of research has found a within-subjects PREE, while another has found a within-subjects reversed PREE (RPREE). These opposing findings constitute what is considered the most important problem of PREE for theoreticians to explain. Here, we provide a neurocomputational account of the PREE, which helps to reconcile these seemingly contradictory findings of within-subjects experimental conditions. The performance of our model demonstrates how omission expectancy, learned according to low probability reward, comes to control response choice following discontinuation of reward presentation (extinction). We find that a PREE will occur when multiple responses become controlled by omission expectation in extinction, but not when only one omission-mediated response is available. Our model exploits the affective states of reward acquisition and reward omission expectancy in order to differentially classify stimuli and differentially mediate response choice. We demonstrate that stimulusâ\\x80\\x93response (retrospective) and stimulusâ\\x80\\x93expectationâ\\x80\\x93response (prospective) routes are required to provide a necessary and sufficient explanation of the PREE versus RPREE data and that Omission representation is key for explaining the nonlinear nature of extinction data.'}], 'doi-url': 'http://dx.doi.org/10.1007/s00422-017-0730-1', 'instances': [{'url': 'http://link.springer.com/article/10.1007/s00422-017-0730-1/fulltext.html', 'provenance': 'CrossRef', 'access-rights': 'UNKNOWN'}, {'url': 'http://link.springer.com/content/pdf/10.1007/s00422-017-0730-1.pdf', 'provenance': 'CrossRef', 'access-rights': 'UNKNOWN'}, {'url': 'http://link.springer.com/content/pdf/10.1007/s00422-017-0730-1.pdf', 'provenance': 'CrossRef', 'access-rights': 'UNKNOWN'}], 'authors': [{'affiliations': [{'official-page': 'http://www.gu.se/', 'provenance': 'MAG', 'identifiers': [{'value': 'http://en.wikipedia.org/wiki/University_of_Gothenburg', 'schema': 'wikpedia'}, {'value': 'grid.8761.8', 'schema': 'grid.ac'}], 'value': 'University of Gothenburg'}], 'given': 'Robert', 'identifiers': [{'provenance': 'CrossRef', 'value': 'http://orcid.org/0000-0002-0307-3171', 'schema': None}, {'provenance': 'MAG', 'value': 'https://academic.microsoft.com/#/detail/2100594078', 'schema': 'URL'}], 'fullname': 'Robert Lowe', 'family': 'Lowe'}, {'affiliations': [{'official-page': 'http://www.gu.se/', 'provenance': 'MAG', 'identifiers': [{'value': 'http://en.wikipedia.org/wiki/University_of_Gothenburg', 'schema': 'wikpedia'}, {'value': 'grid.8761.8', 'schema': 'grid.ac'}], 'value': 'University of Gothenburg'}], 'given': 'Alexander', 'identifiers': [{'provenance': 'MAG', 'value': 'https://academic.microsoft.com/#/detail/2066553684', 'schema': 'URL'}], 'fullname': 'Alexander Almér', 'family': 'Almér'}, {'affiliations': [{'official-page': 'https://www.his.se/', 'provenance': 'MAG', 'identifiers': [{'value': 'http://en.wikipedia.org/wiki/University_of_Skövde', 'schema': 'wikpedia'}, {'value': 'grid.412798.1', 'schema': 'grid.ac'}], 'value': 'University of Skövde'}], 'given': 'Erik', 'identifiers': [{'provenance': 'MAG', 'value': 'https://academic.microsoft.com/#/detail/2341282900', 'schema': 'URL'}], 'fullname': 'Erik Billing', 'family': 'Billing'}, {'affiliations': [{'official-page': 'https://www.ethz.ch/en.html', 'provenance': 'MAG', 'identifiers': [{'value': 'http://en.wikipedia.org/wiki/ETH_Zurich', 'schema': 'wikpedia'}, {'value': 'grid.5801.c', 'schema': 'grid.ac'}], 'value': 'ETH Zurich'}], 'given': 'Yulia', 'identifiers': [{'provenance': 'MAG', 'value': 'https://academic.microsoft.com/#/detail/2744091096', 'schema': 'URL'}], 'fullname': 'Yulia Sandamirskaya', 'family': 'Sandamirskaya'}, {'affiliations': [{'official-page': 'http://www.lunduniversity.lu.se/', 'provenance': 'MAG', 'identifiers': [{'value': 'http://en.wikipedia.org/wiki/Lund_University', 'schema': 'wikpedia'}, {'value': 'grid.4514.4', 'schema': 'grid.ac'}], 'value': 'Lund University'}], 'given': 'Christian', 'identifiers': [{'provenance': 'MAG', 'value': 'https://academic.microsoft.com/#/detail/134376634', 'schema': 'URL'}], 'fullname': 'Christian Balkenius', 'family': 'Balkenius'}], 'collectedFrom': ['CrossRef', 'MAG'], 'accepted': None, 'type': 'journal-article', 'published-online': '2017-9-14', 'subject': ['Biotechnology', 'General Computer Science']}\n"
     ]
    }
   ],
   "source": [
    "metadata = metadata_retriever(metadatadb.doiboost2017, '10.1007/s00422-017-0730-1')\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GRID id retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_ids_retriever(metadata):\n",
    "    \"\"\"\n",
    "    Uses metadata to retrieve the GRID ids\n",
    "    \n",
    "    @param  dict    The metadata of a paper\n",
    "    @return array   The list of GRID ids    \n",
    "    \"\"\"\n",
    "    \n",
    "    # initial list\n",
    "    gridIDs = []\n",
    "    \n",
    "    # get the author information\n",
    "    authors = metadata['authors']\n",
    "    \n",
    "    # loop over the authors in the list\n",
    "    for author in authors:\n",
    "        \n",
    "        # get the affiliation(s) of the author\n",
    "        affiliations = author['affiliations']\n",
    "        \n",
    "        # only continue if there is information about the affiliation\n",
    "        if affiliations:\n",
    "            \n",
    "            # loop over the affiliation information\n",
    "            for affiliation in affiliations:\n",
    "                \n",
    "                # get the identifiers\n",
    "                identifiers = affiliation['identifiers']\n",
    "                \n",
    "                # get the value of the second item, which is always the GRID id\n",
    "                gridIDs.append(identifiers[1]['value'])\n",
    "                \n",
    "    # return the list of affiliations\n",
    "    return gridIDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the GRID ids that belong to the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grid.8761.8', 'grid.8761.8', 'grid.412798.1', 'grid.5801.c', 'grid.4514.4']\n"
     ]
    }
   ],
   "source": [
    "gridIDs = grid_ids_retriever(metadata)\n",
    "print(gridIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the GRID index of Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_host = \"elasticsnarcis\"\n",
    "es_local = Elasticsearch([es_host])\n",
    "searchindex = 'grid'\n",
    "doctype = 'metadata'\n",
    "es = es_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(gridIDs):\n",
    "    \"\"\"\n",
    "    Classify a research as Dutch/Non-Dutch based on the grid ids\n",
    "    \n",
    "    @param  array    The grid IDs of the authors\n",
    "    @return boolean  True for Dutch, False for Non-Dutch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get GRID data\n",
    "    for ID in gridIDs:\n",
    "        res = es.search(index=searchindex, doc_type=doctype, body={\"query\": {\"match\": {'ID': \"%s\" % ID }}})\n",
    "        country = res['hits']['hits'][0]['_source']['Country']\n",
    "        \n",
    "        # Verify Dutch affiliation\n",
    "        if country == 'Netherlands':\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "classification = classify(gridIDs)\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The complete pipeline from DOI to Dutch/Non-Dutch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DOIBoost_pipeline(collection, DOIs):\n",
    "    \"\"\"\n",
    "    The complete pipeline for identifying Dutch research using DOIBoost\n",
    "    @param  Collection    The DOIBoost collection in MongoDB\n",
    "    @param  array         The list of DOIs that need to be identified\n",
    "    @return dict          A dictionary with {DOI: classification} pairs\n",
    "    \"\"\"\n",
    "    \n",
    "    # resulting dict\n",
    "    result = {}\n",
    "    \n",
    "    # loop over the DOIs\n",
    "    for DOI in DOIs:\n",
    "        \n",
    "        # retrieve the metadata\n",
    "        metadata = metadata_retriever(collection, DOI)\n",
    "        \n",
    "        # retrieve the GRID ids\n",
    "        gridIDs = grid_ids_retriever(metadata)\n",
    "        \n",
    "        # skip if we don't have any GRID ids and can't be sure about the DOI identification\n",
    "        if gridIDs:\n",
    "            \n",
    "            # classify the DOI based on the GRID ids\n",
    "            classification = classify(gridIDs)\n",
    "            \n",
    "            # add the classification to the resulting dict\n",
    "            result[DOI] = classification\n",
    "    \n",
    "    # return the result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10.1007/s00422-017-0730-1': False}\n"
     ]
    }
   ],
   "source": [
    "classifications = DOIBoost_pipeline(metacollection, ['10.1007/s00422-017-0730-1'])\n",
    "print(classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(classifications, golden):\n",
    "    \"\"\"\n",
    "    Evaluate the classifier\n",
    "    \n",
    "    @param  dict   The labels from the classifier\n",
    "    @param  dict   The golden standard labels\n",
    "    @return dict   The True Positives, False Positives, True Negatives and False Negatives\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    result = defaultdict(int)\n",
    "    \n",
    "    for DOI in classifications:\n",
    "        classification = classifications[DOI]\n",
    "        gold = golden[DOI]\n",
    "        \n",
    "        if gold == True:\n",
    "            \n",
    "            if classification == True:\n",
    "                result['TP'] += 1\n",
    "            else: \n",
    "                result['FN'] += 1\n",
    "        else:\n",
    "            \n",
    "            if classification == False:\n",
    "                result['TN'] += 1\n",
    "            else: \n",
    "                result['FP'] += 1\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'TN': 1})\n"
     ]
    }
   ],
   "source": [
    "# this will always return true positives and true negatives\n",
    "evaluations = evaluation(classifications, classifications)\n",
    "print(evaluations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
